{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:25%\"><img src='https://www.np.edu.sg/PublishingImages/Pages/default/odp/ICT.jpg' style=\"width: 250px; height: 125px; \"></th>\n",
    "        <th style=\"text-align:center;\"><h1>Deep Learning</h1><h2>Practical 8b - Text generation with LSTM</h2><h3>AY2020/21 Semester</h3></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras:  2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "print('keras: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. Run through Section 1: [Demo: Implementing character-level LSTM text generation](#demo)\n",
    "2. Reflect on what you have learned from this topic in Section 2: [Reflection](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Demo: Implementing character-level LSTM text generation  <a id='demo' />\n",
    "\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preparing the data\n",
    "\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 6s 10us/step\n",
      "Corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of \n",
    "shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot \n",
    "encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200281\n",
      "Unique characters: 59\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building the network\n",
    "\n",
    "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. But let us note that \n",
    "recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in \n",
    "recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lji6\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training the language model and sampling from it\n",
    "\n",
    "\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, \n",
    "and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures \n",
    "after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of \n",
    "temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:From C:\\Users\\lji6\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 200281 samples\n",
      "200281/200281 [==============================] - 142s 709us/sample - loss: 2.0218\n",
      "--- Generating with seed: \"men of antiquity, who knew by their own schooling\n",
      "how to app\"\n",
      "------ temperature: 0.2\n",
      "men of antiquity, who knew by their own schooling\n",
      "how to apparing the promes the comperience the man in the belief of the conceration of the pristion of the probles of the case the ender to the ender to the mant and and the probles to the sent the superioration of the sent to the can the compority of the present of the comperience to the force of the comporing the master the comperity of the sentical the the comperity of the conceration of the comperience \n",
      "------ temperature: 0.5\n",
      "cal the the comperity of the conceration of the comperience to german as and one the force that im the just with the ear in the and the self the will one that in the moranity and stance of it is and the fore and things the contectly as the praction of the serit the the cancer, and the voution of a\n",
      "comperience the despors of means the more and or and so nat under and degrand of a comparhable and and light to a sting to most compirations man the fartes of pe\n",
      "------ temperature: 1.0\n",
      "d light to a sting to most compirations man the fartes of perhip und the baintment the cimcute,\n",
      "and make ware the simpo.\n",
      "imare, stant by of  naithtelhs, mo-flect in allet, and fuect toough, mage things. mostice of\n",
      "a with finit mannidiallitits make vine preitian\n",
      "thric it man the prince more tone stall pitery linghination of scoultt\n",
      "and an endine, have what the feak forny to grongerssor to philomogys, the becicated most hie untect, makand to\n",
      "his pised forgut\n",
      "------ temperature: 1.2\n",
      "s, the becicated most hie untect, makand to\n",
      "his pised forgutionty, nurgew,\n",
      "to work be that he thnse peryen nor under withir dasperity\n",
      "nords takr-litity? in convidity\" on of\n",
      "degilinly: mean's is interedation great r mand and hool, witthe  grace suffnionsllefue may casesk \"dong,\n",
      "the veltimeririty; a devouderh reitle luavn thereiould to\n",
      "one know,nic xim valuatanges, has spope frictiog, sme\n",
      "bepartinsm.ty, the everyfulouinty. igac saccity of voorces, lladking\n",
      "e\n",
      "epoch 2\n",
      "Train on 200281 samples\n",
      "200281/200281 [==============================] - 139s 692us/sample - loss: 1.6383\n",
      "--- Generating with seed: \"instead of being virtues, would be the waste of virtues: so \"\n",
      "------ temperature: 0.2\n",
      "instead of being virtues, would be the waste of virtues: so the self man and such a say and saint that it is the same and and and man and conscience and and fact man fance of the self all that the present to the present of the spirit and present the self conscience of the self cannot that it is the same and the seases and all the self all the self all the really and conscience and the sense and the self in the same and the self man and the fact of the self\n",
      "------ temperature: 0.5\n",
      "e self in the same and the self man and the fact of the self the great to the saint false work of the man and the depticianing, and truenal the any and the eash was said and upon the best\n",
      "of all the nestayment of even the self conscience, the repudious in all its be really as the \"man and all things falses of consciences and and man has to them and with a their the undifferent man in even and does have not in the fund to the same that they man has that\n",
      "som\n",
      "------ temperature: 1.0\n",
      " have not in the fund to the same that they man has that\n",
      "something trimpleis unsforminand, these innecisglodnalism, what wan-appray it not their feeling, whom\n",
      "by should conspect of the condind his fundays to there are subbadhing to boers any presinuried tower man moralitued, and not been come praination--so  hemansed, i without unman. especially dain that be dislamant comandfully in \"faith time which his have courage. the science as egois earificity, to ca\n",
      "------ temperature: 1.2\n",
      "ich his have courage. the science as egois earificity, to cants--langly man-without how man, bratter, trutt, windsis.-\n",
      "-highwriotohameny arizers, puttil there even them therese onoming,.ame itself of himsness. thonjorion, becreatoalss--is such a allocieus evingfuanious faim-svenes pre: af it acoodvesed youcling, beviners of other, day, much deabow noy at little to that inotionslest ranking raded just, ththeris deally dnewly\n",
      "oraagings and gaie it oevily--ag\n",
      "epoch 3\n",
      "Train on 200281 samples\n",
      "200281/200281 [==============================] - 173s 865us/sample - loss: 1.5466\n",
      "--- Generating with seed: \"on should not be regarded as an\n",
      "enduring whole from which an\"\n",
      "------ temperature: 0.2\n",
      "on should not be regarded as an\n",
      "enduring whole from which and desirations of the problem of the spirit and something and destruct to the sense of the morality of the sense of the stand and stand and sense of the sense of the spirit, and say, the spirit\n",
      "and sold and stands of the sense and decised of the spirit of the spirit, the problem to the sense of the sense of the most problem and soul of the man is instinct and destruct of the sense of the morality o\n",
      "------ temperature: 0.5\n",
      " man is instinct and destruct of the sense of the morality of contrated to seem of the simples to be and ender and and present the degrame of the good more the power\n",
      "and happiness and religions and stands of self a feat in instance with the man is who last of consequently lower problem of spirits and solound the considerly and possessed and the problem the charmed to pire if in the such also the ideas, the warder, the morality, the expections to a problem \n",
      "------ temperature: 1.0\n",
      "deas, the warder, the morality, the expections to a problem theter-delish failthance, \"for that may enrover of consigerus onestlife: andipate owed man,\n",
      "the bedies\n",
      "if\n",
      "account conceiving feelings--has not or or fauseness, ruage, as \"burlest in his clut\n",
      "addlinevers, we which passe of perhaps, \"why ethord, which\n",
      "possibly of leist for netthracition. and man, will joy, deficved, however in dis and fetherene, the fames--the clasact besident seed at that we fect\n",
      "d\n",
      "------ temperature: 1.2\n",
      "rene, the fames--the clasact besident seed at that we fect\n",
      "dangest may a fainiby experuianceforish\n",
      "when\n",
      "mo, to syen-a sotrtationifaunt\" to myt\n",
      "itbeaty, fapt of\n",
      "dence of the\n",
      "viringologid\n",
      "ho possification, mertareis,\n",
      "mood rarity cormanted dake in, something donou\n",
      "eutacim encessedly of dange, to his have not eefyminate and kind to antso undarhacy of\n",
      "perfoution muchor\n",
      "enough,, do, anything. thee hedsergone,\n",
      "dnee i\n",
      "pangeed in the popicion hands to wanting\n",
      "plose\n",
      "epoch 4\n",
      "Train on 200281 samples\n",
      "200281/200281 [==============================] - 322s 2ms/sample - loss: 1.4994\n",
      "--- Generating with seed: \"-as idealized by the socialistic fools and\n",
      "shallow-pates--th\"\n",
      "------ temperature: 0.2\n",
      "-as idealized by the socialistic fools and\n",
      "shallow-pates--the surted to the sure the such the such a something the present the surtions of the instinct to the something and the such a subtle and and the such the instinct of the fact the such an instance and delight to the something the belief to the something the surtists and the something the surting the subjuged the surting to the spirit to the something man in the construct to the fearly the good to the\n",
      "------ temperature: 0.5\n",
      "something man in the construct to the fearly the good to the exters and one of the sense to a the humsely, and the present to the surt of the soul of the and instance of the instinct in the such the opposes of the man and distrust to the such the sart of the man--a the european and desire the spirit about an europe and the abstrute has\n",
      "nothing to all the former the will the something and subtle the most suffering the actions of one of the enjoy the self-su\n",
      "------ temperature: 1.0\n",
      "e most suffering the actions of one of the enjoy the self-sute inventitudus, way jubeilie any\n",
      "pay sostens, nome the agances, so its whold generations of the spire considered limake it with over-rurgrative the goerness, hitherto existences.\n",
      "but themselves ourselves,\" withergh 'reverd\n",
      "of great, in more caumimone, is only balsion in this has the costess. history,\" and one anrarity is tet\n",
      "generate them to not man and\n",
      "evil\n",
      "that it allegatearty in them the own\n",
      "s\n",
      "------ temperature: 1.2\n",
      "m to not man and\n",
      "evil\n",
      "that it allegatearty in them the own\n",
      "sense peter\n",
      "indifferent of amobur; in all its earlry is christian) has astul, a pricamed with\n",
      "kinious\n",
      "beffifit\n",
      "stands tangemonation. but judgualtiy and \"mas, douber-gard\n",
      "madave alrematings of\n",
      "qualitude now! but\n",
      "the gets'het in the rarigruse in the in lacks that in it, happest conknim oried opcan oprr\n",
      "of previnent we appies to end one sror.\n",
      "a\n",
      "divgrgrow from our sufficmone; on you thgication not--an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in \n",
    "particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text \n",
    "becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \n",
    "\"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings \n",
    "of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment \n",
    "with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and \n",
    "realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is \n",
    "sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is \n",
    "a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To \n",
    "evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like \n",
    "our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic \n",
    "statistical structure, thus making it impossible to learn a language model like we just did.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reflection  <a id='ref' />\n",
    "Provide a reflection on what you have learned from this topic below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have learn many thing this lesson. First is using the numpy to download data from online.\n",
    "I also learn how to create a Long short-term memory system(LSTM) neural network. LSTM allow us to process a sequences of data. The heigher the temperature we set for the LSTM model, the will generate and more interesting, surprising, even creative but it may start to create words that is not real english words, so we need to balence the temperature to have the best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
