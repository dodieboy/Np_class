{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:25%\"><img src='https://www.np.edu.sg/PublishingImages/Pages/default/odp/ICT.jpg' style=\"width: 250px; height: 125px; \"></th>\n",
    "        <th style=\"text-align:center;\"><h1>Deep Learning</h1><h2>Practical 3b - Overfitting and Underfitting</h2><h3>AY2020/21 Semester</h3></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras:  2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "print('keras: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "After completing this practical exercise, students should be able to:\n",
    "1. [Understand how to prevent overfitting in Neural Networks](#over)\n",
    "2. [Exercise - Develop your own model and try to avoid overfitting](#exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in this notebook we will be using the IMDB test set as our validation set. It doesn't matter in this context.\n",
    "\n",
    "Let's prepare the data using the code from Practical 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to prevent overfitting in Neural Networks <a id='over' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reducing the network's size \n",
    "\n",
    "Our original network was as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "original_model = models.Sequential()\n",
    "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "original_model.add(layers.Dense(16, activation='relu'))\n",
    "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "original_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to replace it with this smaller network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "smaller_model.add(layers.Dense(4, activation='relu'))\n",
    "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "smaller_model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 145us/sample - loss: 0.4500 - acc: 0.8268 - val_loss: 0.3355 - val_acc: 0.8819\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.2608 - acc: 0.9088 - val_loss: 0.2841 - val_acc: 0.8890\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.2014 - acc: 0.9293 - val_loss: 0.2852 - val_acc: 0.8870\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.1670 - acc: 0.9415 - val_loss: 0.2934 - val_acc: 0.8838\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.1448 - acc: 0.9494 - val_loss: 0.3200 - val_acc: 0.8776\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.1257 - acc: 0.9561 - val_loss: 0.3814 - val_acc: 0.8612\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.1117 - acc: 0.9613 - val_loss: 0.3579 - val_acc: 0.8721\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0990 - acc: 0.9665 - val_loss: 0.3793 - val_acc: 0.8699\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.0881 - acc: 0.9698 - val_loss: 0.4100 - val_acc: 0.8657\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0763 - acc: 0.9747 - val_loss: 0.4561 - val_acc: 0.8586\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0685 - acc: 0.9779 - val_loss: 0.4624 - val_acc: 0.8623\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.0588 - acc: 0.9822 - val_loss: 0.5240 - val_acc: 0.8570\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 2s 81us/sample - loss: 0.0516 - acc: 0.9839 - val_loss: 0.5278 - val_acc: 0.8582\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s 81us/sample - loss: 0.0438 - acc: 0.9875 - val_loss: 0.5616 - val_acc: 0.8588\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0387 - acc: 0.9885 - val_loss: 0.5961 - val_acc: 0.8565\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0345 - acc: 0.9905 - val_loss: 0.6311 - val_acc: 0.8545\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 2s 80us/sample - loss: 0.0278 - acc: 0.9921 - val_loss: 0.6656 - val_acc: 0.8533\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.0235 - acc: 0.9939 - val_loss: 0.7219 - val_acc: 0.8484\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0196 - acc: 0.9952 - val_loss: 0.7790 - val_acc: 0.8491\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.0175 - acc: 0.9956 - val_loss: 0.7980 - val_acc: 0.8484\n"
     ]
    }
   ],
   "source": [
    "original_hist = original_model.fit(x_train, y_train,\n",
    "                                   epochs=20,\n",
    "                                   batch_size=512,\n",
    "                                   validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 109us/sample - loss: 0.5992 - acc: 0.7374 - val_loss: 0.5259 - val_acc: 0.8147\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.4438 - acc: 0.8807 - val_loss: 0.4109 - val_acc: 0.8770\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.3370 - acc: 0.9067 - val_loss: 0.3415 - val_acc: 0.8859\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.2702 - acc: 0.9176 - val_loss: 0.3048 - val_acc: 0.8894\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.2275 - acc: 0.9272 - val_loss: 0.2871 - val_acc: 0.8912\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.1988 - acc: 0.9345 - val_loss: 0.2896 - val_acc: 0.8846\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.1775 - acc: 0.9412 - val_loss: 0.2824 - val_acc: 0.8879\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.1605 - acc: 0.9463 - val_loss: 0.2856 - val_acc: 0.8871\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.1476 - acc: 0.9512 - val_loss: 0.3061 - val_acc: 0.8796\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2s 81us/sample - loss: 0.1361 - acc: 0.9555 - val_loss: 0.3029 - val_acc: 0.8820\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.1268 - acc: 0.9584 - val_loss: 0.3216 - val_acc: 0.8773\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.1180 - acc: 0.9624 - val_loss: 0.3259 - val_acc: 0.8780\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 2s 81us/sample - loss: 0.1103 - acc: 0.9654 - val_loss: 0.3377 - val_acc: 0.8754\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s 80us/sample - loss: 0.1030 - acc: 0.9677 - val_loss: 0.3502 - val_acc: 0.8742\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0966 - acc: 0.9704 - val_loss: 0.3738 - val_acc: 0.8691\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.0904 - acc: 0.9725 - val_loss: 0.3933 - val_acc: 0.8671\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 2s 76us/sample - loss: 0.0852 - acc: 0.9750 - val_loss: 0.3978 - val_acc: 0.8676\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0803 - acc: 0.9760 - val_loss: 0.4128 - val_acc: 0.8656\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 2s 80us/sample - loss: 0.0754 - acc: 0.9783 - val_loss: 0.4335 - val_acc: 0.8645\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0704 - acc: 0.9806 - val_loss: 0.4442 - val_acc: 0.8635\n"
     ]
    }
   ],
   "source": [
    "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
    "                                       epochs=20,\n",
    "                                       batch_size=512,\n",
    "                                       validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of \n",
    "the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance \n",
    "degrades much more slowly once it starts overfitting.\n",
    "\n",
    "Now, for kicks, let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model = models.Sequential()\n",
    "bigger_model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "bigger_model.add(layers.Dense(512, activation='relu'))\n",
    "bigger_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "bigger_model.compile(optimizer='rmsprop',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 136us/sample - loss: 0.4540 - acc: 0.8048 - val_loss: 0.3156 - val_acc: 0.8700\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 111us/sample - loss: 0.2152 - acc: 0.9151 - val_loss: 0.2851 - val_acc: 0.8861\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 113us/sample - loss: 0.1211 - acc: 0.9566 - val_loss: 0.3766 - val_acc: 0.8795\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 3s 112us/sample - loss: 0.0619 - acc: 0.9812 - val_loss: 0.4692 - val_acc: 0.8771\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 112us/sample - loss: 0.0967 - acc: 0.9860 - val_loss: 0.4731 - val_acc: 0.8826\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 112us/sample - loss: 0.0021 - acc: 0.9998 - val_loss: 0.6848 - val_acc: 0.8807\n",
      "Epoch 7/20\n",
      "12800/25000 [==============>...............] - ETA: 0s - loss: 3.5657e-04 - acc: 0.9999"
     ]
    }
   ],
   "source": [
    "bigger_model_hist = bigger_model.fit(x_train, y_train,\n",
    "                                     epochs=20,\n",
    "                                     batch_size=512,\n",
    "                                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the bigger network fares compared to the reference one. The dots are the validation loss values of the bigger network, and the \n",
    "crosses are the initial network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model_val_loss = bigger_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, bigger_model_val_loss, 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also \n",
    "more noisy.\n",
    "\n",
    "Meanwhile, here are the training losses for our two networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_loss = original_hist.history['loss']\n",
    "bigger_model_train_loss = bigger_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, bigger_model_train_loss, 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be \n",
    "able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large \n",
    "difference between the training and validation loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Adding weight regularization\n",
    "\n",
    "In Keras, weight regularization is added by passing _weight regularizer instances_ to layers as keyword arguments. Let's add L2 weight regularization to our movie review classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "l2_model = models.Sequential()\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu', input_shape=(10000,)))\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu'))\n",
    "l2_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l2(0.001)` means that every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of the network. \n",
    "\n",
    "Here's the impact of our L2 regularization penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model_hist = l2_model.fit(x_train, y_train,\n",
    "                             epochs=20,\n",
    "                             batch_size=512,\n",
    "                             validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), \n",
    "even though both models have the same number of parameters.\n",
    "\n",
    "As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# L1 regularization\n",
    "regularizers.l1(0.001)\n",
    "\n",
    "# L1 and L2 regularization at the same time\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Adding dropout\n",
    "\n",
    "In Keras you can introduce dropout in a network via the `Dropout` layer, which gets applied to the output of layer right before it. Let's add two `Dropout` layers in our IMDB network to see how well they do at reducing overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model = models.Sequential()\n",
    "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(16, activation='relu'))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "dpt_model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
    "                               epochs=20,\n",
    "                               batch_size=512,\n",
    "                               validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Again, a clear improvement over the reference network.\n",
    "\n",
    "To recap: here the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Getting more training data.\n",
    "* Reducing the capacity of the network.\n",
    "* Adding weight regularization.\n",
    "* Adding dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise <a id='exc' />\n",
    "Please develop your own model for IMDB review classification problem following \"The Universal Workflow of Machine Learning\" and try to maximize the validation accuracy. When you are regularizing the model you can utilize the below tecniques:\n",
    "* Add dropout\n",
    "* Try different architectures: add or remove layers, increase or decrease the number of units per layer\n",
    "* Add L1 and / or L2 regularization\n",
    "* Adjust the hyperparameters, e.g. the learning rate of the optimizer, the number of epochs, batch_size and etc.\n",
    "\n",
    "Provide your codes and comments in the below boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Build and Compile the model. \n",
    "# Remember to use the above mentioned regularization techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Compile and Fit the model\n",
    "# You can adjust the number of epochs, the batch size, the learning rate of the optimizer and etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Plot the training and validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Comments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
